# Value-Iteration-Interface
![demo](https://github.com/zt55699/Value-Iteration-Interface/blob/main/interface.png)
Markov Decision Processes
This material is about making decisions in stochastic environments, namely making decisions that incorporate utility and uncertainty. It is based on the Rusell and Norvig's book, "Artificial Intelligence: A Modern Approach." Our running example is an agent in a grid world, see Fig. 1. There are 12 squares in a grid, each represented by a state. The agent is initially in state (1,1) and wants to go to state (4,3) where it receives a positive reward of +1. If the agent ends up in state (4,2) it receives a negative reward of ô€€€1. States (4,3) and (4,2) are terminal states; no move is allowed once the agent reaches them and the whole process needs to restart again. State (1,1) is called start state. State (2,2) is unreachable; it is a barrier where the agent cannot go. The set of actions the agent can execute in a state s is denoted by A(s). In this example the A(s) for each state is fUp, Down, Left, Rightg. We abbreviate them by U;D;R;L. If the agent tries to move in a direction and there is a wall, the agent remains in the same state. If the environment were deterministic, then the solution for going from state (1,1) to (4,3) is easy: U;U;R;R;R. However, actions are unreliable. Each action achieves the intended eect with probability 0.8. The rest of the time the agent moves at a right angle to the intended direction. For example the agent wants to go Up but can end up moving Left with probability 0.1 or Right with probability also 0.1. If there is a wall in the direction the agent moves, the agent stays in the same state.

![demo](https://github.com/zt55699/Value-Iteration-Interface/blob/main/algorithm.png)
